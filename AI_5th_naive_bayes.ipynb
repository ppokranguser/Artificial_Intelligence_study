{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ppokranguser/Artificial_Intelligence_study/blob/main/AI_5th_naive_bayes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Naive Bayes Classifier Example with PyTorch\n",
        "\n",
        "In this example, we will implement a Naive Bayes classifier using PyTorch for a spam detection task on the SMS Spam Collection dataset. PyTorch does not have a built-in Naive Bayes implementation, so we will manually construct the classifier by calculating the necessary probabilities.\n",
        "\n",
        "\n",
        "## Step 0. Import Libraries and Prepare the Dataset\n",
        "\n",
        "- Download the following dataset and upload it into the current session.\n",
        "SMS Spam Dataset: [https://www.kaggle.com/datasets/uciml/sms-spam-collection-dataset]\n"
      ],
      "metadata": {
        "id": "zlbFaiiHCNcp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "from google.colab import files\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "id": "oej0dhDYC9It"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1. Load and Preprocess the Dataset\n",
        "\n",
        "We need to convert the text messages into a bag-of-words representation using CountVectorizer and convert them into PyTorch tensors.\n",
        "\n",
        "SMS Spam dataset consists of two columnes: v1 and v2\n",
        "- v1: Label (ham or spam)\n",
        "- v2: raw text\n"
      ],
      "metadata": {
        "id": "4DN-za5BJLDS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eBiBkeL0CArD"
      },
      "outputs": [],
      "source": [
        "# Load the dataset\n",
        "df = pd.read_csv('/content/drive/MyDrive/spam.csv', encoding='latin1')\n",
        "print(\"The number of examples\", len(df)) # 5,572\n",
        "\n",
        "df = df[['v1', 'v2']] # v1: label, v2: raw text\n",
        "print(df.head(10)) # Display the first 10 rows\n",
        "\n",
        "# Initialize the CountVectorizer to transform text into a bag-of-words model\n",
        "# Convert the messages into numeric form\n",
        "vectorizer = CountVectorizer(stop_words='english')\n",
        "X = vectorizer.fit_transform(df['v2']).toarray()\n",
        "\n",
        "# Labels (spam/ham)\n",
        "y = df['v1'].map({'ham': 0, 'spam': 1}).values  # Map ham to 0 and spam to 1"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2. Split the Dataset\n",
        "\n",
        "We will split the dataset into training and test sets and convert them to PyTorch tensors.\n",
        "- Training set: used for training the bayes classifier\n",
        "- Test set: used for evaluting the trained classifier"
      ],
      "metadata": {
        "id": "_Z7o80vQKTzb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the dataset into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
        "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
        "y_test_tensor = torch.tensor(y_test, dtype=torch.float32)"
      ],
      "metadata": {
        "id": "Ei0bJsOVKQ-c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3. Build the Naive Bayes Classifier using PyTorch\n",
        "\n",
        "In PyTorch, we will manually implement the Naive Bayes classifier by calculating the prior probabilities and likelihoods for each feature given the class.\n",
        "- P(Y): The prior probability of each class (spam/ham) is computed from the training data.\n",
        "- P(X|Y): The likelihood of each feature (word presence/absence) given the class is computed.\n",
        "\n",
        "(**Self-study**): Laplace smoothing is applied to handle zero probabilities.\n",
        "- Zero frequency problem: What if a new word appears in the test set? Naive Bayes Classifier always predict zero probability for a new data that has never been observed... How can we address this problem?\n"
      ],
      "metadata": {
        "id": "wEfit3bGKcE6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NaiveBayesClassifier:\n",
        "    def __init__(self):\n",
        "        self.class_probs = None\n",
        "        self.feature_probs_given_class = None\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        # Calculate the prior probability P(Y) for each class\n",
        "        class_counts = torch.bincount(y.long())\n",
        "        self.class_probs = class_counts / torch.sum(class_counts)\n",
        "\n",
        "        # Calculate the likelihood P(X|Y) for each feature given the class\n",
        "        feature_counts_given_class = torch.zeros((len(class_counts), X.shape[1]))\n",
        "\n",
        "        for c in range(len(class_counts)):\n",
        "            feature_counts_given_class[c] = torch.sum(X[y == c], axis=0)\n",
        "\n",
        "        # Laplace smoothing\n",
        "        # (Self-study): how can this technique address the issue of zero probabilities?\n",
        "        self.feature_probs_given_class = (feature_counts_given_class + 1) / (\n",
        "            torch.sum(feature_counts_given_class, axis=1, keepdim=True) + 2\n",
        "        )\n",
        "\n",
        "    def predict(self, X):\n",
        "        # Calculate log probabilities to avoid numerical underflow\n",
        "        log_class_probs = torch.log(self.class_probs)\n",
        "        log_feature_probs_given_class = torch.log(self.feature_probs_given_class)\n",
        "        log_probs = torch.matmul(X, log_feature_probs_given_class.T) + log_class_probs\n",
        "        return torch.argmax(log_probs, axis=1)"
      ],
      "metadata": {
        "id": "qTBf8tLHKkAE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Train the Model\n",
        "\n",
        "- Now that we have defined the model, we will train it on the training data.\n",
        "- We can train the Naive Bayes Classifier via only a single scan of the dataset."
      ],
      "metadata": {
        "id": "7SjmOvyoLIp3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the model\n",
        "nb_classifier = NaiveBayesClassifier()\n",
        "\n",
        "# Train the model\n",
        "nb_classifier.fit(X_train_tensor, y_train_tensor)"
      ],
      "metadata": {
        "id": "HPJkdgOOLNfM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Make Predictions and Evaluation\n",
        "- After training the model, we can make predictions on the test data.\n",
        "- Convert the predictions back to NumPy arrays for evaluation.\n",
        "\n",
        "\n",
        "### Accuracy\n",
        "- The accuracy shows how many predictions were correct out of the total predictions. A high accuracy indicates that the model is correctly classifying messages as spam or ham.\n",
        "\n",
        "### Classification Report\n",
        "The classification report provides detailed insights into the model's performance:\n",
        "\n",
        "- Precision: The ratio of true positive predictions (correctly predicted spam) to all predicted positives.\n",
        "- Recall: The ratio of true positives to all actual positives (how well the model identifies spam).\n",
        "- F1-Score: The harmonic mean of precision and recall, offering a balanced measure of the model's performance.\n"
      ],
      "metadata": {
        "id": "nFOTGoi3LbgN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions on the test data\n",
        "y_pred_tensor = nb_classifier.predict(X_test_tensor)\n",
        "\n",
        "# Convert predictions to NumPy arrays\n",
        "y_pred = y_pred_tensor.numpy()\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
        "\n",
        "# Detailed classification report\n",
        "print(classification_report(y_test, y_pred, target_names=['ham', 'spam']))"
      ],
      "metadata": {
        "id": "1sAMYiVKLkvJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Summary\n",
        "\n",
        "In this hands-on practice, we implemented a Naive Bayes classifier using PyTorch for text classification.\n",
        "\n",
        "We covered:\n",
        "- Preprocessing the SMS Spam Collection dataset.\n",
        "- Building a Naive Bayes classifier from scratch in PyTorch.\n",
        "- Training the model on the preprocessed data.\n",
        "- Evaluating the model's performance using metrics like accuracy and F1-score.\n",
        "\n",
        "Although PyTorch does not have built-in support for Naive Bayes, we constructed the classifier manually by computing the necessary probabilities. This practice illustrates the flexibility and power of PyTorch for custom machine learning models."
      ],
      "metadata": {
        "id": "kDoRNwmXMBgp"
      }
    }
  ]
}